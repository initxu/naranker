arch: "NAR"

exp_name: "prj4096_din512_lr007_dm128_4head"
seed: 20211117

is_cuda: True

space: "nasbench201"
network_type: "cifar100"

train_size: 1000
val_size: 250
drop_last: True
data_loader_workers: 6
batch_size: 64

start_epochs: 0
ranker_epochs: 100
sampler_epochs: 72
validate_freq: 2

strategy: "val_acc"

bins: 5

ranker:
  n_tier: 5
  n_arch_patch: 31
  d_patch: 4
  d_patch_vec: 128
  d_model: 128
  d_ffn_inner: 512
  d_tier_prj_inner: 512
  n_layers: 6
  n_head: 4
  d_k: 32
  d_v: 32
  dropout: 0.1
  n_position: 200
  scale_prj: True

optimizer:
  beta1: 0.9 
  beta2: 0.99
  eps: 1.0e-9
  weight_decay: 1.0e-2 # AdamW默认是1e-2, 这是因为直接将L2正则加在了二阶动量上，可以尝试默认值, tranfomer复现代码是5.0e-4

lr_scheduler:
  lr_mul: 0.07
  n_warmup_steps: 300

sampler:
  top_tier: 1
  last_tier: 4 # 其实是从计数3到4，即最后两个tier
  batch_factor: 0.1
  reuse_step: 25 # for 25 batch 128, 12 for batch 64
  max_trails: 100
  noisy_factor: 0.1
  flops_kl_thred: 0.35
  params_kl_thred: 0.3
  edges_kl_thred: 1.5
  sample_size: 64
  search_size: 64
  force_uniform: True    # for params and flops sample, n_nodes are always uniform
  is_checkpoint: True
